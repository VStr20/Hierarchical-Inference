{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet18, resnet50\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from memory_profiler import memory_usage\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install memory-profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunny/CCRL_vs/env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sunny/CCRL_vs/env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X0_5_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(root='/home/sunny/CCRL_vs/Hierarchical_Inference/Datasets/Imagenet1K/val', transform=transform_test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = models.shufflenet_v2_x0_5(pretrained=True)  # Use pretrained weights on ImageNet-1k\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "num_images = len(test_dataset)  # Get the number of images\n",
    "values = np.linspace(0, 1, 256)\n",
    "thresholds = [round(i, 4) for i in values]\n",
    "\n",
    "num_thresholds = len(thresholds)\n",
    "\n",
    "def quantize_to_8bit(probabilities):\n",
    "    # Ensure probabilities are in the [0, 1] range\n",
    "    probabilities = torch.clamp(probabilities, 0, 1)\n",
    "    \n",
    "    # Scale probabilities to 255 intervals, round to the nearest one, and then scale back\n",
    "    quantized = torch.round(probabilities * 255) / 255\n",
    "    \n",
    "    return quantized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 121.49406599998474\n",
      "Memory Used: 291.59 MB\n",
      "Cost: 1123.599999999987\n",
      "Accuracy 93.16849349987254\n",
      "2139 268 3923\n"
     ]
    }
   ],
   "source": [
    "mem_before = memory_usage()[0]\n",
    "start = time.time()\n",
    "# Initialize the dictionaries for UCB, LCB, Tx, Fx\n",
    "dict = {\n",
    "    'UCB': {threshold: 1.0 for threshold in thresholds},  # Upper confidence bound\n",
    "    'LCB': {threshold: 0.0 for threshold in thresholds},  # Lower confidence bound\n",
    "    'Tx': {threshold: 0 for threshold in thresholds},      # Count of times each threshold has been reached\n",
    "    'Fx': {threshold: 0 for threshold in thresholds},    # Fraction of correct predictions\n",
    "}\n",
    "\n",
    "# print([dict['UCB'][i] for i in thresholds])\n",
    "\n",
    "correct_counts = {threshold: 0 for threshold in thresholds}\n",
    "offloads_counts = {threshold: 0 for threshold in thresholds}\n",
    "total_counts = {threshold: 0 for threshold in thresholds}\n",
    "incorrect_counts = {threshold: 0 for threshold in thresholds}\n",
    "count = 1\n",
    "\n",
    "# Set tau and delta for thresholding\n",
    "tau = 0.4  # Example value, set according to your requirements\n",
    "delta = 1/math.e  # Example value, set according to your requirements\n",
    "cost = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # print(\"Image number\", count)\n",
    "        # print()\n",
    "        \n",
    "        # Get model outputs and softmax probabilities\n",
    "        outputs = model(images)\n",
    "        softmax_probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        quantized_probs = quantize_to_8bit(softmax_probs)\n",
    "\n",
    "        predicted_probs, predicted_labels = torch.max(quantized_probs, dim=1)\n",
    "        pred_probs = round(predicted_probs.item(), 4)\n",
    "        # print(\"SML output\", pred_probs)\n",
    "\n",
    "        pred_class = predicted_labels.item()\n",
    "        # print(\"predicted class\", pred_class)\n",
    "\n",
    "        true_class = labels.item()\n",
    "        # print(pred_class, true_class)\n",
    "\n",
    "        total_counts[pred_probs] += 1\n",
    "\n",
    "        if dict['LCB'][pred_probs] >= (1 - tau):\n",
    "            # print(\"UCB of predicted class\", dict['UCB'][pred_probs])\n",
    "            # print(\"LCB of predicted class is\", dict['LCB'][pred_probs])\n",
    "            # print(\"Accept, nothing learnt here\")\n",
    "            if pred_class == true_class:\n",
    "                continue\n",
    "            else:\n",
    "                cost += 1\n",
    "                incorrect_counts[pred_probs] += 1\n",
    "\n",
    "        elif dict['UCB'][pred_probs] <= (1 - tau):\n",
    "            dict['Tx'][pred_probs] += 1\n",
    "            # print(\"UCB of predicted class\", dict['UCB'][pred_probs])\n",
    "            # print(\"LCB of predicted class\", dict['LCB'][pred_probs])\n",
    "            # print(\"Offload\")\n",
    "            if pred_class == true_class:\n",
    "                correct_counts[pred_probs] += 1\n",
    "            dict['Fx'][pred_probs] = correct_counts[pred_probs] / dict['Tx'][pred_probs]\n",
    "            cost += tau\n",
    "            offloads_counts[pred_probs] += 1\n",
    "\n",
    "        elif dict['UCB'][pred_probs] > (1 - tau) and dict['LCB'][pred_probs] < (1 - tau):\n",
    "            dict['Tx'][pred_probs] += 1\n",
    "            # print(\"UCB of predicted class\", dict['UCB'][pred_probs])\n",
    "            # print(\"LCB of predicted class\", dict['LCB'][pred_probs])\n",
    "            # print(\"Offload\")\n",
    "            if pred_class == true_class:\n",
    "                correct_counts[pred_probs] += 1\n",
    "            dict['Fx'][pred_probs] = correct_counts[pred_probs] / dict['Tx'][pred_probs]\n",
    "            cost += tau\n",
    "            offloads_counts[pred_probs] += 1\n",
    "\n",
    "        log_delta = math.log(1/delta)\n",
    "        uncertainty = math.sqrt(log_delta / (dict['Tx'][pred_probs] + 1e-8))  # Added small value to avoid division by zero\n",
    "\n",
    "        dict['UCB'][pred_probs] = min(1, dict['Fx'][pred_probs] + uncertainty)\n",
    "        dict['LCB'][pred_probs] = max(0, dict['Fx'][pred_probs] - uncertainty)\n",
    "        # print(uncertainty)\n",
    "        # print(dict['UCB'][pred_probs], dict['LCB'][pred_probs])\n",
    "        \n",
    "        for x1 in thresholds:\n",
    "            if x1 <= pred_probs:\n",
    "                for x in thresholds:\n",
    "                    if(x1 <= x and x <= pred_probs):\n",
    "                        dict['UCB'][x1] = min(dict['UCB'][x1], dict['UCB'][x])\n",
    "\n",
    "        for x2 in thresholds:\n",
    "            if x2 >= pred_probs:\n",
    "                for x in thresholds:\n",
    "                    if(x2 >= x and x >= pred_probs):\n",
    "                        dict['LCB'][x2] = max(dict['LCB'][x2], dict['LCB'][x])\n",
    "                    \n",
    "        count += 1\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time Taken: {end - start}\")\n",
    "mem_after = memory_usage()[0]\n",
    "print(f\"Memory Used: {mem_after - mem_before:.2f} MB\")\n",
    "  \n",
    "incorrect_sum = 0\n",
    "total_sum = 0\n",
    "offload_sum = 0\n",
    "for i in incorrect_counts:\n",
    "    incorrect_sum += incorrect_counts[i]\n",
    "    total_sum += total_counts[i]\n",
    "    offload_sum += offloads_counts[i]\n",
    "\n",
    "print(f\"Cost: {cost}\")\n",
    "\n",
    "# print(f\"Original Accuracy\", accuracy)\n",
    "print(f\"Accuracy\", (total_sum - incorrect_sum)*100/total_sum)\n",
    "\n",
    "print(offload_sum, incorrect_sum, total_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.8611111111111\n",
      "80.23600000000002\n",
      "1122.44\n"
     ]
    }
   ],
   "source": [
    "accuracies = [95.18,\n",
    "97.17,\n",
    "94.64,\n",
    "90.03,\n",
    "96.35,\n",
    "95.69,\n",
    "94.82,\n",
    "96.71,\n",
    "93.16\n",
    "]\n",
    "time_list = [74.47, 72.45, 60.90, 71.87, 121.49]\n",
    "cost_list = [1129.39, 1111.79, 1124.99, 1123.59]\n",
    "cost_average = 0.0\n",
    "average_accuracy = 0\n",
    "time_average = 0.0\n",
    "for i in accuracies:\n",
    "    average_accuracy += i\n",
    "for i in time_list:\n",
    "    time_average += i\n",
    "for i in cost_list:\n",
    "    cost_average += i\n",
    "\n",
    "print(average_accuracy/len(accuracies))\n",
    "print(time_average/len(time_list))\n",
    "print(cost_average/len(cost_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 41.69509696960449\n",
      "Memory Used: -4.71 MB\n",
      "Cost: 1202.1999999999987\n",
      "Accuracy 98.19016059138414\n",
      "2828 71 3923\n"
     ]
    }
   ],
   "source": [
    "mem_before = memory_usage()[0]\n",
    "\n",
    "start = time.time()\n",
    "# Initialize the dictionaries for UCB, LCB, Tx, Fx\n",
    "dict = {\n",
    "    'UCB': {threshold: 1.0 for threshold in thresholds},  # Upper confidence bound\n",
    "    'LCB': {threshold: 0.0 for threshold in thresholds},  # Lower confidence bound\n",
    "    'Tx': {threshold: 0 for threshold in thresholds},      # Count of times each threshold has been reached\n",
    "    'Fx': {threshold: 0 for threshold in thresholds},    # Fraction of correct predictions\n",
    "}\n",
    "\n",
    "# print([dict['UCB'][i] for i in thresholds])\n",
    "\n",
    "correct_counts = {threshold: 0 for threshold in thresholds}\n",
    "offloads_counts = {threshold: 0 for threshold in thresholds}\n",
    "total_counts = {threshold: 0 for threshold in thresholds}\n",
    "incorrect_counts = {threshold: 0 for threshold in thresholds}\n",
    "count = 1\n",
    "\n",
    "# Set tau and delta for thresholding\n",
    "tau = 0.4  # Example value, set according to your requirements\n",
    "delta = 1/math.e  # Example value, set according to your requirements\n",
    "cost = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # print(\"Image number\", count)\n",
    "        # print()\n",
    "        \n",
    "        # Get model outputs and softmax probabilities\n",
    "        outputs = model(images)\n",
    "        softmax_probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        quantized_probs = quantize_to_8bit(softmax_probs)\n",
    "\n",
    "        predicted_probs, predicted_labels = torch.max(quantized_probs, dim=1)\n",
    "        pred_probs = round(predicted_probs.item(), 4)\n",
    "        # print(\"SML output\", pred_probs)\n",
    "\n",
    "        pred_class = predicted_labels.item()\n",
    "        # print(\"predicted class\", pred_class)\n",
    "\n",
    "        true_class = labels.item()\n",
    "        # print(pred_class, true_class)\n",
    "\n",
    "        total_counts[pred_probs] += 1\n",
    "\n",
    "        if dict['LCB'][pred_probs] >= (1 - tau):\n",
    "            # print(\"UCB of predicted class\", dict['UCB'][pred_probs])\n",
    "            # print(\"LCB of predicted class is\", dict['LCB'][pred_probs])\n",
    "            # print(\"Accept, nothing learnt here\")\n",
    "            if pred_class == true_class:\n",
    "                continue\n",
    "            else:\n",
    "                cost += 1\n",
    "                incorrect_counts[pred_probs] += 1\n",
    "\n",
    "        elif dict['UCB'][pred_probs] <= (1 - tau):\n",
    "            dict['Tx'][pred_probs] += 1\n",
    "            # print(\"UCB of predicted class\", dict['UCB'][pred_probs])\n",
    "            # print(\"LCB of predicted class\", dict['LCB'][pred_probs])\n",
    "            # print(\"Offload\")\n",
    "            if pred_class == true_class:\n",
    "                correct_counts[pred_probs] += 1\n",
    "            dict['Fx'][pred_probs] = correct_counts[pred_probs] / dict['Tx'][pred_probs]\n",
    "            cost += tau\n",
    "            offloads_counts[pred_probs] += 1\n",
    "\n",
    "        elif dict['UCB'][pred_probs] > (1 - tau) and dict['LCB'][pred_probs] < (1 - tau):\n",
    "            dict['Tx'][pred_probs] += 1\n",
    "            # print(\"UCB of predicted class\", dict['UCB'][pred_probs])\n",
    "            # print(\"LCB of predicted class\", dict['LCB'][pred_probs])\n",
    "            # print(\"Offload\")\n",
    "            if pred_class == true_class:\n",
    "                correct_counts[pred_probs] += 1\n",
    "            dict['Fx'][pred_probs] = correct_counts[pred_probs] / dict['Tx'][pred_probs]\n",
    "            cost += tau\n",
    "            offloads_counts[pred_probs] += 1\n",
    "\n",
    "        log_delta = math.log(1/delta)\n",
    "        uncertainty = math.sqrt(log_delta / (dict['Tx'][pred_probs] + 1e-8))  # Added small value to avoid division by zero\n",
    "\n",
    "        dict['UCB'][pred_probs] = min(1, dict['Fx'][pred_probs] + uncertainty)\n",
    "        dict['LCB'][pred_probs] = max(0, dict['Fx'][pred_probs] - uncertainty)\n",
    "        \n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time Taken: {end - start}\")\n",
    "\n",
    "mem_after = memory_usage()[0]\n",
    "print(f\"Memory Used: {mem_after - mem_before:.2f} MB\")\n",
    "\n",
    "incorrect_sum = 0\n",
    "total_sum = 0\n",
    "offload_sum = 0\n",
    "for i in incorrect_counts:\n",
    "    incorrect_sum += incorrect_counts[i]\n",
    "    total_sum += total_counts[i]\n",
    "    offload_sum += offloads_counts[i]\n",
    "\n",
    "print(f\"Cost: {cost}\")\n",
    "\n",
    "# print(f\"Original Accuracy\", accuracy)\n",
    "print(f\"Accuracy\", (total_sum - incorrect_sum)*100/total_sum)\n",
    "\n",
    "print(offload_sum, incorrect_sum, total_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.04666666666665\n",
      "41.24666666666666\n",
      "1200.67\n"
     ]
    }
   ],
   "source": [
    "accuracies = [97.57,\n",
    "97.85,\n",
    "98.21,\n",
    "97.96,\n",
    "98.29,\n",
    "98.03,\n",
    "98.16,\n",
    "98.16,\n",
    "98.19\n",
    "]\n",
    "time_list = [41.24, 41.43, 40.19, 42.43, 40.50, 41.69]\n",
    "cost_list = [1205.79, 1199.99, 1196.39, 1198.99, 1202.19]\n",
    "cost_average = 0.0\n",
    "average_accuracy = 0\n",
    "time_average = 0.0\n",
    "for i in accuracies:\n",
    "    average_accuracy += i\n",
    "for i in time_list:\n",
    "    time_average += i\n",
    "for i in cost_list:\n",
    "    cost_average += i\n",
    "\n",
    "print(average_accuracy/len(accuracies))\n",
    "print(time_average/len(time_list))\n",
    "print(cost_average/len(cost_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 84.75924897193909\n",
      "Memory Used: 5.77 MB\n",
      "Cost: 1140.7999999999895\n",
      "Accuracy 92.30180983940862\n",
      "2097 302 3923\n"
     ]
    }
   ],
   "source": [
    "mem_before = memory_usage()[0]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Algorithm - Hedge\n",
    "def choose_threshold(weights, thresholds, W_t):\n",
    "    # Create probability distribution for selecting a threshold\n",
    "    probabilities = np.array([weights[theta] / W_t for theta in thresholds])\n",
    "    \n",
    "    # Choose a threshold based on the weight distribution\n",
    "    chosen_threshold_idx = np.random.choice(len(thresholds), p=probabilities)\n",
    "    chosen_threshold = thresholds[chosen_threshold_idx]\n",
    "    \n",
    "    return chosen_threshold\n",
    "\n",
    "# Iterate over different values of alpha\n",
    "for alpha in np.arange(0, 0.5, 0.4):\n",
    "    η = 0.02 / alpha**(1/3) if alpha > 0 else 0.02\n",
    "    ε = min(1, math.sqrt(η / (2 * alpha))) if alpha > 0 else 0.9\n",
    "    cost = 0\n",
    "\n",
    "    weights = {threshold: 1.0 for threshold in thresholds}  # Initialize weights for each threshold\n",
    "    W_t = sum(weights.values())  # Sum the initial weights to get W_t\n",
    "    incorrect_count = {threshold: 0 for threshold in thresholds}\n",
    "    correct_count = {threshold: 0 for threshold in thresholds}\n",
    "    offload_count = {threshold: 0 for threshold in thresholds}\n",
    "    total_count = {threshold: 0 for threshold in thresholds}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get model outputs and softmax probabilities\n",
    "            outputs = model(images)\n",
    "            softmax_probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            # Quantize probabilities to 8-bit\n",
    "            quantized_probs = quantize_to_8bit(softmax_probs)\n",
    "\n",
    "            # Get the predicted probabilities and classes\n",
    "            predicted_probs, predicted_labels = torch.max(quantized_probs, dim=1)\n",
    "            p_t = round(predicted_probs.item(), 4)\n",
    "\n",
    "            pred_class = predicted_labels.item()\n",
    "            true_class = labels.item()\n",
    "\n",
    "            # Choose a threshold based on the current weights\n",
    "            chosen_threshold = choose_threshold(weights, thresholds, W_t)\n",
    "\n",
    "            total_count[p_t] += 1\n",
    "\n",
    "            if p_t < chosen_threshold:\n",
    "                offload_count[p_t] += 1\n",
    "                cost += tau\n",
    "            else:\n",
    "                if pred_class != true_class:\n",
    "                    incorrect_count[p_t] += 1\n",
    "                    cost += 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # Compute pseudo-loss and update weights\n",
    "            if pred_class == true_class:\n",
    "                Y_t = 0\n",
    "            else:\n",
    "                Y_t = 1\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                if p_t < threshold:\n",
    "                    pseudo_loss = alpha  # Offload cost\n",
    "                else:\n",
    "                    pseudo_loss = Y_t / ε if np.random.binomial(1, ε) else 0  # Loss depends on epsilon\n",
    "\n",
    "                # Update the weight for the current threshold\n",
    "                weights[threshold] = weights[threshold] * np.exp(-η * pseudo_loss)\n",
    "\n",
    "            # Recalculate W_t as the sum of all updated weights\n",
    "            W_t = sum(weights.values())\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time Taken: {end - start}\")\n",
    "\n",
    "mem_after = memory_usage()[0]\n",
    "print(f\"Memory Used: {mem_after - mem_before:.2f} MB\")\n",
    "\n",
    "incorrect_sum = 0\n",
    "total_sum = 0\n",
    "offload_sum = 0\n",
    "for i in incorrect_counts:\n",
    "    incorrect_sum += incorrect_count[i]\n",
    "    total_sum += total_count[i]\n",
    "    offload_sum += offload_count[i]\n",
    "\n",
    "print(f\"Cost: {cost}\")\n",
    "\n",
    "# print(f\"Original Accuracy\", accuracy)\n",
    "print(f\"Accuracy\", (total_sum - incorrect_sum)*100/total_sum)\n",
    "\n",
    "print(offload_sum, incorrect_sum, total_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.33999999999999\n",
      "84.705\n",
      "1132.5233333333333\n"
     ]
    }
   ],
   "source": [
    "accuracies = [92.14, 92.58, 92.30]\n",
    "time_list = [84.01, 84.91, 85.74, 85.08, 83.74, 84.75]\n",
    "cost_list = [1127.19, 1131.79, 1131.19, 1136.79, 1127.39, 1140.79]\n",
    "cost_average = 0.0\n",
    "average_accuracy = 0\n",
    "time_average = 0.0\n",
    "for i in accuracies:\n",
    "    average_accuracy += i\n",
    "for i in time_list:\n",
    "    time_average += i\n",
    "for i in cost_list:\n",
    "    cost_average += i\n",
    "\n",
    "print(average_accuracy/len(accuracies))\n",
    "print(time_average/len(time_list))\n",
    "print(cost_average/len(cost_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 61.033531665802\n",
      "Memory Used: -3.61 MB\n",
      "Cost: 1194.199999999997\n",
      "Accuracy 97.88427224063217\n",
      "2778 83 3923\n"
     ]
    }
   ],
   "source": [
    "mem_before = memory_usage()[0]\n",
    "start = time.time()\n",
    "# Initialize the dictionaries for UCB, LCB, Tx, Fx\n",
    "dict = {\n",
    "    'UCB': {threshold: 1.0 for threshold in thresholds},  # Upper confidence bound\n",
    "    'LCB': {threshold: 0.0 for threshold in thresholds},  # Lower confidence bound\n",
    "    'Tx': {threshold: 0 for threshold in thresholds},      # Count of times each threshold has been reached\n",
    "    'Fx': {threshold: 0 for threshold in thresholds},    # Fraction of correct predictions\n",
    "}\n",
    "\n",
    "# print([dict['UCB'][i] for i in thresholds])\n",
    "\n",
    "correct_counts = {threshold: 0 for threshold in thresholds}\n",
    "offloads_counts = {threshold: 0 for threshold in thresholds}\n",
    "total_counts = {threshold: 0 for threshold in thresholds}\n",
    "incorrect_counts = {threshold: 0 for threshold in thresholds}\n",
    "count = 1\n",
    "\n",
    "# Set tau and delta for thresholding\n",
    "tau = 0.4  # Example value, set according to your requirements\n",
    "delta = 1/math.e  # Example value, set according to your requirements\n",
    "cost = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # print(\"Image number\", count)\n",
    "        # print()\n",
    "        \n",
    "        # Get model outputs and softmax probabilities\n",
    "        outputs = model(images)\n",
    "        softmax_probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        quantized_probs = quantize_to_8bit(softmax_probs)\n",
    "\n",
    "        predicted_probs, predicted_labels = torch.max(quantized_probs, dim=1)\n",
    "        pred_probs = round(predicted_probs.item(), 4)\n",
    "        # print(\"SML output\", pred_probs)\n",
    "\n",
    "        pred_class = predicted_labels.item()\n",
    "        # print(\"predicted class\", pred_class)\n",
    "\n",
    "        true_class = labels.item()\n",
    "        # print(pred_class, true_class)\n",
    "\n",
    "        total_counts[pred_probs] += 1\n",
    "\n",
    "        if dict['LCB'][pred_probs] >= (1 - tau):\n",
    "            # print(\"UCB of predicted class\", dict['UCB'][pred_probs])\n",
    "            # print(\"LCB of predicted class is\", dict['LCB'][pred_probs])\n",
    "            # print(\"Accept, nothing learnt here\")\n",
    "            if pred_class == true_class:\n",
    "                continue\n",
    "            else:\n",
    "                cost += 1\n",
    "                incorrect_counts[pred_probs] += 1\n",
    "\n",
    "        elif dict['UCB'][pred_probs] <= (1 - tau):\n",
    "            dict['Tx'][pred_probs] += 1\n",
    "            # print(\"UCB of predicted class\", dict['UCB'][pred_probs])\n",
    "            # print(\"LCB of predicted class\", dict['LCB'][pred_probs])\n",
    "            # print(\"Offload\")\n",
    "            if pred_class == true_class:\n",
    "                correct_counts[pred_probs] += 1\n",
    "            dict['Fx'][pred_probs] = correct_counts[pred_probs] / dict['Tx'][pred_probs]\n",
    "            cost += tau\n",
    "            offloads_counts[pred_probs] += 1\n",
    "\n",
    "        elif dict['UCB'][pred_probs] > (1 - tau) and dict['LCB'][pred_probs] < (1 - tau):\n",
    "            dict['Tx'][pred_probs] += 1\n",
    "            # print(\"UCB of predicted class\", dict['UCB'][pred_probs])\n",
    "            # print(\"LCB of predicted class\", dict['LCB'][pred_probs])\n",
    "            # print(\"Offload\")\n",
    "            if pred_class == true_class:\n",
    "                correct_counts[pred_probs] += 1\n",
    "            dict['Fx'][pred_probs] = correct_counts[pred_probs] / dict['Tx'][pred_probs]\n",
    "            cost += tau\n",
    "            offloads_counts[pred_probs] += 1\n",
    "\n",
    "        log_delta = math.log(1/delta)\n",
    "        uncertainty = math.sqrt(log_delta / (dict['Tx'][pred_probs] + 1e-8))  # Added small value to avoid division by zero\n",
    "\n",
    "        dict['UCB'][pred_probs] = min(1, dict['Fx'][pred_probs] + uncertainty)\n",
    "        dict['LCB'][pred_probs] = max(0, dict['Fx'][pred_probs] - uncertainty)\n",
    "        # print(uncertainty)\n",
    "        # print(dict['UCB'][pred_probs], dict['LCB'][pred_probs])\n",
    "        \n",
    "        eta = max(0.2, min(0.25, uncertainty))  # Dynamically adjust epsilon\n",
    "\n",
    "        for x1 in thresholds:\n",
    "            if x1 <= pred_probs:\n",
    "                for x in thresholds:\n",
    "                    if x1 <= x and x <= pred_probs:\n",
    "                        if uncertainty < eta:\n",
    "                            dict['UCB'][x1] = min(dict['UCB'][x1], dict['UCB'][x])\n",
    "\n",
    "        for x2 in thresholds:\n",
    "            if x2 >= pred_probs:\n",
    "                for x in thresholds:\n",
    "                    if x2 >= x and x >= pred_probs:\n",
    "                        if uncertainty < eta:\n",
    "                            dict['LCB'][x2] = max(dict['LCB'][x2], dict['LCB'][x])\n",
    "\n",
    "                    \n",
    "        count += 1\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time Taken: {end - start}\")\n",
    "mem_after = memory_usage()[0]\n",
    "print(f\"Memory Used: {mem_after - mem_before:.2f} MB\")\n",
    "  \n",
    "incorrect_sum = 0\n",
    "total_sum = 0\n",
    "offload_sum = 0\n",
    "for i in incorrect_counts:\n",
    "    incorrect_sum += incorrect_counts[i]\n",
    "    total_sum += total_counts[i]\n",
    "    offload_sum += offloads_counts[i]\n",
    "\n",
    "print(f\"Cost: {cost}\")\n",
    "\n",
    "# print(f\"Original Accuracy\", accuracy)\n",
    "print(f\"Accuracy\", (total_sum - incorrect_sum)*100/total_sum)\n",
    "\n",
    "print(offload_sum, incorrect_sum, total_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.9875\n",
      "61.135000000000005\n",
      "1197.44\n"
     ]
    }
   ],
   "source": [
    "accuracies = [98.44, 97.75, 97.83, 97.93, 98.08]\n",
    "time_list = [61.51, 60.95, 60.95, 61.13, 60.22]\n",
    "cost_list = [1201.79, 1198.79, 1192.59, 1196.59, 1196.19]\n",
    "cost_average = 0.0\n",
    "average_accuracy = 0\n",
    "time_average = 0.0\n",
    "for i in accuracies:\n",
    "    average_accuracy += i\n",
    "for i in time_list:\n",
    "    time_average += i\n",
    "for i in cost_list:\n",
    "    cost_average += i\n",
    "\n",
    "print(average_accuracy/len(accuracies))\n",
    "print(time_average/len(time_list))\n",
    "print(cost_average/len(cost_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eta = max(0.1, min(0.05, uncertainty))  # Dynamically adjust epsilon\n",
    "<br>\n",
    "Time Taken: 61.69447326660156    <br>\n",
    "Memory Used: -11.68 MB    <br>\n",
    "Cost: 1195.7999999999959    <br>\n",
    "Accuracy 97.88427224063217    <br>\n",
    "2782 83 3923\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
